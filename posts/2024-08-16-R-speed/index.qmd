---
title: "R and `data.table` are very fast"
date: "2024-08-18"
knitr:
  opts_chunk:
      echo: true
      # eval: true
      cache: true
      warning: false
      message: false
      comment:  # removes ## from output
      dev: svglite
draft: true
# citations-hover: true
# footnotes-hover: true
---

I work with fairly large data locally on a laptop, and R with `data.table` is surprisingly quick compared to alternatives. The two operations which usually take longest, reading in data and grouped lags, are quicker than Julia or Python.^[As always, there's probably ways to speed up those other languages, but I've done the recommended approach for each from googling.]

```{r}
#| hide: true
#| code-fold: true
#| code-summary: "Setup"
library(tidyverse)
library(data.table)

library(duckdb)
library(dbplyr)
library(reticulate)
library(JuliaCall)

setDTthreads(8)
set.seed(123)

# nrows <- 1e7 # big-ish NOTE: 5e7 at end
nrows <- 5e7 # big-ish NOTE: 5e7 at ed
round_to <- nrows / 10 # On average 10x observations per ID
big_data_set <- data.table(
  ID = round(runif(nrows, 1, round_to), 0),
  y = rnorm(nrows)
)
# arrange(ID)

tmp_dir <- tempdir()
tmp_file <- paste0(tmp_dir, "/big_data.csv")

# scipen so it doesn't use 1e9 which polars can't parse
fwrite(big_data_set, tmp_file, scipen = 999)
```

I've made a file with a value and a group identifier which is `r file.size(tmp_file) |> utils:::format.object_size("auto")`. Now for each language I:

1. Read in the data.

2. Add a column which is the lag of `y` grouped by the ID column, which is already sorted for fairness.

::: {.panel-tabset}

## R (data.table)

```{r}
load_time_data.table <- system.time({
  big_data_set <- fread(tmp_file)
})

# Is this cheating?
# setkey(big_data_set, ID)

shift_time_data.table <- system.time({
  big_data_set[, lag_y := shift(y, 1), keyby = ID]
})
```

## R (data.table with index)

```{r}
load_time_data.table_index <- system.time({
  big_data_set <- fread(tmp_file)
})

setkey(big_data_set, ID)

shift_time_data.table_index <- system.time({
  big_data_set[, lag_y := shift(y, 1), keyby = ID]
})
```

## R (dplyr)

```{r}
#| eval: false
load_time_tidyverse <- system.time({
  big_data_set <- readr::read_csv(tmp_file)
})

shift_time_tidyverse <- system.time({
  big_data_set <- big_data_set |>
    dplyr::group_by(ID) |>
    dplyr::mutate(lag_y = dplyr::lag(y))
})
```
<!-- I don't have all day
## Base R

```{r}
#| eval: false
load_time_base_r <- system.time({
  big_data_set <- read.csv(tmp_file)
})

shift_time_base_r <- system.time({
  big_data_set$lag_y <- ave(
    big_data_set$y,
    big_data_set$ID,
    FUN = function(x) c(NA, x[-length(x)])
  )
})
```
-->

## Python (pandas)

```{python}
import time
import pandas as pd

t0 = time.time()
big_data_set = pd.read_csv(r.tmp_file, engine="pyarrow")
t1 = time.time()

load_time_python = t1-t0

t0 = time.time()
big_data_set['lag_y'] = big_data_set.groupby('ID')['y'].shift(1)
t1 = time.time()
shift_time_python = t1 - t0
```

## Python (pandas but sorted)

```{python}
import time
import pandas as pd

t0 = time.time()
big_data_set = pd.read_csv(r.tmp_file, engine="pyarrow")
t1 = time.time()

load_time_python = t1-t0

# Sort
big_data_set.sort_values('ID')

t0 = time.time()
big_data_set['lag_y'] = big_data_set.groupby('ID')['y'].shift(1)
t1 = time.time()
shift_time_python = t1 - t0
```

## Python (polars)

```{python}
import time
import polars as pl

t0 = time.time()
big_data_set = pl.read_csv(r.tmp_file) #, dtypes = {"ID": pl.Int64, "y": pl.Float64})
t1 = time.time()
load_time_polars = t1 - t0

t0 = time.time()
big_data_set = big_data_set.with_columns(
    pl.col('y').shift(1).over('ID').alias('lag_y')
)
t1 = time.time()
shift_time_polars = t1 - t0
```

## Julia

```{julia}
#| output: false
using RCall
using CSV
using DataFrames
using ShiftedArrays
import ShiftedArrays: lag

@rget tmp_file

load_time_julia = @elapsed begin
    df = CSV.read(tmp_file, DataFrame)
end

shift_time_julia = @elapsed begin
    df = transform(groupby(df, :ID), :y => lag => :y_lag)
end

# Do it twice, because Julia is quicker second time
load_time_julia2 = @elapsed begin
    df = CSV.read(tmp_file, DataFrame)
end

shift_time_julia2 = @elapsed begin
    df = transform(groupby(df, :ID), :y => lag => :y_lag)
end
```

## DuckDB

```{r}
big_data_set <- fread(tmp_file)

con <- dbConnect(
  duckdb(),
  dbdir = ":memory:"
)

dbWriteTable(
  con,
  name = "big_data_set",
  value = big_data_set
)

# Load for consistency, although this doesn't make much sense with a database
load_time_duckdb <- system.time({
  big_data_set <- tbl(
    con,
    "big_data_set"
  ) |>
    collect()
})

shift_time_duckdb <- system.time({
  tbl(
    con,
    "big_data_set"
  ) |>
    dplyr::group_by(ID) |>
    dplyr::mutate(lag_y = dplyr::lag(y)) |>
    collect()
    # Could time this with `collect()` as well
})

dbDisconnect(con, shutdown = TRUE)
```

:::

# Results

```{r}
#| include: false

# Get Python and Julia times
load_time_python <- py$load_time_python
shift_time_python <- py$shift_time_python
load_time_polars <- py$load_time_polars
shift_time_polars <- py$shift_time_polars
shift_time_julia <- julia_eval("shift_time_julia")
load_time_julia <- julia_eval("load_time_julia")
shift_time_julia2 <- julia_eval("shift_time_julia2")
load_time_julia2 <- julia_eval("load_time_julia2")

timings <- tibble::tribble(
  ~language, ~type, ~time,
  "R (data.table)", "Load Data", load_time_data.table[[3]],
  "R (data.table)", "Group By + Lag", shift_time_data.table[[3]],
  "R (data.table with index)", "Load Data", load_time_data.table_index[[3]],
  "R (data.table with index)", "Group By + Lag", shift_time_data.table_index[[3]],
  # "R (dplyr)", "Load Data", load_time_tidyverse[[3]],
  # "R (dplyr)", "Group By + Lag", shift_time_tidyverse[[3]],
  # "R (base)", "Load Data", load_time_base_r[[3]],
  # "R (base)", "Group By + Lag", shift_time_base_r[[3]],
  "Python (pandas)", "Load Data", as.numeric(load_time_python, units = "secs"),
  "Python (pandas)", "Group By + Lag", as.numeric(shift_time_python, units = "secs"),
  "Python (polars)", "Load Data", as.numeric(load_time_polars, units = "secs"),
  "Python (polars)", "Group By + Lag", as.numeric(shift_time_polars, units = "secs"),
  "Julia (DataFrames)", "Load Data", as.numeric(load_time_julia, units = "secs"),
  "Julia (DataFrames)", "Group By + Lag", as.numeric(shift_time_julia, units = "secs"),
  "Julia 2nd Run (DataFrames)", "Load Data", as.numeric(load_time_julia2, units = "secs"),
  "Julia 2nd Run (DataFrames)", "Group By + Lag", as.numeric(shift_time_julia2, units = "secs"),
  "DuckDB (with R)", "Load Data", load_time_duckdb[[1]],
  "DuckDB (with R)", "Group By + Lag", shift_time_duckdb[[1]]
)
```


```{r}
#| code-fold: true
#| code-summary: "Plot"

timings <- timings |>
  mutate(time = round(time, 2))

color_scheme <- function(...) palette.colors(...) |> unname()
options(ggplot2.discrete.fill = color_scheme(palette = "Tableau 10"))

plot_timings <- timings |>
  dplyr::mutate(total_time = sum(time), .by = language) |>
  dplyr::mutate(relative = time / min(time), .by = type)

ann_text <- data.frame(
  x = nrow(plot_timings) / 2 + 0.3,
  y = 10,
  label = "Time in Seconds",
  type = "Load Data"
)

plot_timings |>
  arrange(type) |>
  ggplot() +
  geom_col(aes(
    x = reorder(language, -total_time),
    y = relative,
    fill = fct_rev(type)
  )) +
  geom_label(
    aes(
      x = language,
      y = relative,
      label = round(time, 2),
    ),
    size = 3
  ) +
  labs(
    title = NULL,
    x = NULL,
    y = "Time (Relative to Slowest)",
    # fill = "Operation"
  ) +
  scale_y_continuous(
    labels = ~ paste0(.x, "x"),
    expand = c(0, 0),
    # limits = c(0, ceiling(max(plot_timings$relative) + 10))
  ) +
  coord_flip() +
  theme_minimal(10) +
  theme(
    legend.position = "none",
    # legend.position = "inside",
    # legend.position.inside = c(0.35, 0.85),
    # axis.line.y = element_line(),
    plot.title.position = "plot",
    panel.grid.minor.y = element_blank(),
    panel.grid.minor.x = element_blank(),
  ) +
  facet_wrap(
    ~ factor(type, levels = c("Load Data", "Group By + Lag")),
    scales = "free"
  ) +
  geom_label(
    aes(
      x = x,
      y = y,
      label = label,
    ),
    data = ann_text,
    size = 3
  )


```

```{r}
#| code-fold: true
#| code-summary: "Table"

timings |>
  tidyr::pivot_wider(
    id_cols = language,
    names_from = type,
    values_from = time
  ) |>
  rename(Language = language) |>
  dplyr::mutate(
    `Total Time` = sum(`Load Data`, `Group By + Lag`) |> round(2),
    .by = Language
  ) |>
  arrange(`Total Time`) |>
  tinytable::tt()
```

`data.table` is a clear winner, although DuckDB and Second run Julia are also up there. `dplyr` is slow, but is not meant to be used for large data: it provides packages to translate to both `data.table` with [dtplyr](https://dtplyr.tidyverse.org/) and to duckdb with [duckplyr](https://dtplyr.tidyverse.org/) or [dbplyr](https://dbplyr.tidyverse.org/).

Initially I thought Python was slow, but you can fix the slow CSV loading with `engine = "pyarrow"`. The group by and lag is very quick on Pandas, but only beats `data.table` if it's sorted by ID beforehand. `data.table` wins if it's unsorted.

These speed comparisons are always a bit theoretical, but it's nice to know that R can hold it's own against what are usually considered faster languages.
